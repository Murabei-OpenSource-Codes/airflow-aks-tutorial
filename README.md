# Airflow K8s with virtual nodes Tutorial
Tutorial to deploy Airflow on Azure AKS with KubernetsPodOperator. It
describes the steps needed to correctly configure the AKS cluster to run
Airflow and permit scheduling tasks on virtual node.

Using virtual node permit scaling resource intensive task on demand,
allocating the necessary resource and realizing them at the end of the task.
Using Airflow with CeleryExecutor along with KubernetsPodOperator permit
mixing simple Python functions with Pod execution.

This tutorial was developed as results of studies and application on different
clients by Murabei Data Science. Murabei is a consulting company from Brazil
dedicated to development of AI and statistical models along with development
of AI oriented systems, for further information check out
[website](https://www.murabei.com).

![alt text](__doc/murabei.png)

# Directory overview
The files of this tutorial are divided in some folders:

## Images
There are two simple docker at test_images subfolders there are two images
that were used on clusters tests. There is also a `login-repository.bash` script
responsible for logging at the docker image repository at Azure and a
pumpwood_images folder with scripts used to transfer some private images
from one repository to another.

The private images are not actually necessary, made the tests possible since
it was build an integration of the Airflow with Pumpood. It is not necessary
to deploy them, but some changes on yml must be necessary.

- **test-task-image:** A test image with a simple loop sleep.
- **test-task-image--error:** A test image with a simple loop sleep, but that
raises an error to test the ability of airflow to deal with error on Pod
execution.

## Deploy
To help creating the yml files with it was used a package to deploy Pumpwood
based systems. It is not necessary to run the those codes, the final yml files
are avaiable at deploy_yml_files.

On this files there are some scripts that helps creating disks, ip and logging
to the cluster on Azure.

- **01__create_public_ip.bash:** Creates an static IP to be used on Load
    Balancer.
- **02__create_disks.bash:** Creates an disks used to persists Postgres data.
- **03__create_fernet_cripto.py:** A simple Python script that can be used
- **04__login_cluster.bash:** Script to login to the cluster and create
    a separeted namespace to deploy the containers.
- **05__clear_cluster.bash:** Clear Kubernets cluster removing all deployments
    and volumes.
- **template__create_docker_secret.bash** A bash script to create the secrets
    necessary for K8s to login on private repository and retrieve images.

## Deploy yml files
Files generated by Pumpwood Deploy to deploy Pods necessary for Airflow.

#### 000__nginx-gateway__endpoint.yml
#### 000__rabbitmq__secrets.yml
#### 001__rabbitmq__deployment.yml
#### 002__hash_salt__secrets.yml
#### 003__storage-config.yml
#### 004__azure__storage_key.yml
#### 005__gcp--storage-key.yml
#### 006__aws__storage_key.yml
#### 007__microsservice_model__secrets.yml
#### 008__load_balancer__volume.yml
#### 009__load_balancer__postgres.yml
#### 010__load_balancer__app.yml
#### 011__nginx-gateway__deploy.yml
#### 012__pumpwood_auth__secrets.yml
#### 013__pumpwood_auth__volume.yml
#### 014__pumpwood_auth__postgres.yml
#### 015__pumpwood_auth_app__deploy.yml
#### 016__pumpwood_auth_admin_static__deploy.yml
#### 017__airflow__secrets.yml
#### 019__airflow__serviceaccount.yml
#### 020__airflow__volume.yml
#### 021__airflow__postgres.yml
#### 022__airflow_app__deploy.yml
#### 023__airflow_scheduler__deploy.yml
#### 024__airflow_worker__deploy.yml

# General configuration comands
```
airflow users create \
  --username admin \
  --firstname FIRST_NAME \
  --lastname LAST_NAME \
  --role Admin \
  --email admin@example.org
```
